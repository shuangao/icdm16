In this paper, we present a novel method called SMTLe that is able to transfer knowledge of the source model in domain adaptation. Inspired by previous work,
we propose a novel perspective on the work of HTL and show the reasons why positive and negative transfer would happen in the different scenario. Then based on our analysis, we propose our method SMTLe that can safely leverage the knowledge from the source models to achieve the improved performance of the target model by limiting the VC dimension of the transfer problem and reduce the empirical risk as well. Experiment results show that SMTLe can leverage related source knowledge and alleviate negative transfer in different scenarios and outperforms other baseline methods.

In our perspective on the domain adaptation problem, the data augmentation approach can fit a wider range of source classifiers. We can leverage the knowledge from any source model that can output the decision score/confidence, such as the Neural Networks and the inference model. There are still some problems to be solve while leveraging the source models with different kinds of classifiers how to better exploit the knowledge to both achieve good positive transfer performance and avoid negative transfer at the same time. This can be an important area in our future work.

