\relax 
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{fei2006one}
\citation{kuzborskij2013stability}
\citation{kuzborskij2013stability}
\citation{tommasi2014learning}
\citation{ben2010theory}
\citation{ben2007analysis}
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\citation{jie2011multiclass}
\citation{Lu201514}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Relying on the related source knowledge can improve the performance of the target model while forcing the target model to rely on the unrelated source could suffer from negative transfer.\relax }}{1}}
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{lim2012transfer}
\citation{LongICML15}
\citation{yang2007cross}
\citation{aytar2011tabula}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Negative transfer happens when we transfer source hypothesis $f'$ to target one. Points with different color represent different categories. The data distribution would change when a new category is added into the dataset. The newly added category (red points) can also greatly affect the data distribution in target task and negative transfer could happen when we consider the multi-class scenario. \relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:distribution}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}}
\newlabel{sec:work}{{II}{2}}
\citation{davis2009deep}
\citation{wang2014active}
\citation{zhou2014multi}
\citation{kuzborskij2013n}
\citation{tommasi2010safety}
\citation{cawley2006leave}
\citation{tommasi2014learning}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\@writefile{toc}{\contentsline {section}{\numberline {III}Transfer knowledge with data augmentation}{3}}
\newlabel{sec:prob}{{III}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Biased regularization in HTL}{3}}
\newlabel{eq:svm_obj}{{1}{3}}
\newlabel{eq:opt}{{2}{3}}
\newlabel{eq:solu}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Data Augumentation in HTL}{3}}
\newlabel{eq:aug_pre}{{4}{3}}
\newlabel{eq:aug}{{5}{3}}
\citation{vapnik1999overview}
\citation{suykens1999least}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The transfer learning process can be considered as the augmentation of the target data where the decision scores of the source models are appended as the auxiliary features. The transfer parameters can be considered as the a part of the corresponding hyperplane. We can consider 2 augmentation strategies: multi-adaptation and single-adaptation.\relax }}{4}}
\newlabel{fig:aug}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Reasons for negative transfer}{4}}
\newlabel{eq:srm}{{6}{4}}
\citation{kuzborskij2013stability}
\citation{elisseeff2003leave}
\citation{cawley2006leave}
\citation{crammer2002algorithmic}
\citation{BoydCO}
\citation{lampert2009learning}
\@writefile{toc}{\contentsline {section}{\numberline {IV}SMTLe}{5}}
\newlabel{sec:smitle}{{IV}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Multiclass Prediction Loss with Leave-One-Out}{5}}
\newlabel{eq:linear}{{7}{5}}
\newlabel{eq:loo}{{8}{5}}
\newlabel{eq:solution}{{10}{5}}
\newlabel{eq:train_loss}{{11}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Loss Function of SMTLe}{5}}
\newlabel{eq:loss}{{12}{5}}
\newlabel{eq:dual}{{13}{5}}
\citation{lecun1998gradient}
\citation{hull1994database}
\citation{jie2011multiclass}
\citation{tommasi2014learning}
\citation{aytar2011tabula}
\citation{kuzborskij2013n}
\citation{tommasi2014learning}
\citation{gehler2009feature}
\newlabel{alg:1}{{\caption@xref {alg:1}{ on input line 1}}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces SMITLe\relax }}{6}}
\newlabel{alg:1}{{1}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiment}{6}}
\newlabel{sec:exp}{{V}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Dataset}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Baseline methods and experiment setup}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Experiment result \& analysis}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Experiment results on 5 examples per class in target training set. We show the percentage of accuracy across the 10 classes on different noise level. We use "*" to denote the results suffer from negative transfer\relax }}{7}}
\newlabel{tab:rs}{{I}{7}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {Results on MNIST}}}{7}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {Results on USPS}}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces We add the noise to the source data to generate the source domain with different relatedness to the target domain. From the images we can see that the source domain is still related to the target domain in different level of the noise rate\relax }}{7}}
\newlabel{fig:noise}{{4}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{7}}
\citation{rockafellar2015convex}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces More results on MNIST\relax }}{8}}
\newlabel{tab:mnist}{{II}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {10 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {15 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(c)}{\ignorespaces {20 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(d)}{\ignorespaces {25 examples per class}}}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces More results on USPS\relax }}{8}}
\newlabel{tab:usps}{{III}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {10 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {15 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(c)}{\ignorespaces {20 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(d)}{\ignorespaces {25 examples per class}}}{8}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  A: Convergence Analysis}{8}}
\newlabel{appd:convg}{{A}{8}}
\newlabel{eq:app:strong}{{15}{8}}
\bibstyle{IEEEtran}
\bibdata{research}
\bibcite{pan2010survey}{1}
\bibcite{tommasi2014learning}{2}
\newlabel{eq:app:inner}{{16}{9}}
\newlabel{eq:app:squrediff}{{17}{9}}
\newlabel{eq:app:it_diff}{{18}{9}}
\newlabel{eq:app:difsum}{{20}{9}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  B: Proof of avoiding negative transfer}{9}}
\newlabel{appd:proof}{{B}{9}}
\newlabel{eq:opt_beta}{{23}{9}}
\newlabel{eq:link1}{{24}{9}}
\bibcite{fei2006one}{3}
\bibcite{kuzborskij2013stability}{4}
\bibcite{ben2010theory}{5}
\bibcite{ben2007analysis}{6}
\bibcite{kuzborskij2013n}{7}
\bibcite{jie2011multiclass}{8}
\bibcite{Lu201514}{9}
\bibcite{lim2012transfer}{10}
\bibcite{LongICML15}{11}
\bibcite{yang2007cross}{12}
\bibcite{aytar2011tabula}{13}
\bibcite{davis2009deep}{14}
\bibcite{wang2014active}{15}
\bibcite{zhou2014multi}{16}
\bibcite{tommasi2010safety}{17}
\bibcite{cawley2006leave}{18}
\bibcite{vapnik1999overview}{19}
\bibcite{suykens1999least}{20}
\bibcite{elisseeff2003leave}{21}
\bibcite{crammer2002algorithmic}{22}
\bibcite{BoydCO}{23}
\bibcite{lampert2009learning}{24}
\bibcite{lecun1998gradient}{25}
\bibcite{hull1994database}{26}
\bibcite{gehler2009feature}{27}
\bibcite{rockafellar2015convex}{28}
\@writefile{toc}{\contentsline {section}{References}{10}}
