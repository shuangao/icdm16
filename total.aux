\relax 
\citation{ben2010theory}
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\citation{kuzborskij2013stability}
\citation{bishop2006pattern}
\citation{tommasi2014learning}
\citation{ben2010theory}
\citation{ben2007analysis}
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\citation{jie2011multiclass}
\citation{Lu201514}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Relying on the related source knowledge can improve the performance of the target model while forcing the target model to rely on the unrelated source could suffer from negative transfer.\relax }}{1}}
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{lim2012transfer}
\citation{LongICML15}
\citation{yang2007cross}
\citation{aytar2011tabula}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Negative transfer happens when we transfer source hypothesis $f'$ to target one. Points with different color represent different categories. The data distribution would change in different domains and negative transfer could happen when we consider the multi-class scenario. \relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:distribution}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}}
\newlabel{sec:work}{{II}{2}}
\citation{davis2009deep}
\citation{wang2014active}
\citation{zhou2014multi}
\citation{kuzborskij2013n}
\citation{tommasi2010safety}
\citation{cawley2006leave}
\citation{vapnik2015learning}
\citation{tommasi2014learning}
\citation{aytar2011tabula}
\citation{tommasi2014learning}
\citation{aytar2011tabula}
\@writefile{toc}{\contentsline {section}{\numberline {III}Transfer knowledge with feature augmentation}{3}}
\newlabel{sec:prob}{{III}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Feature augmentation in HTL}{3}}
\newlabel{eq:aug_pre}{{1}{3}}
\newlabel{eq:svm_obj}{{2}{3}}
\citation{vapnik1999overview}
\citation{suykens1999least}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The transfer learning process can be considered as the augmentation of the target data where the decision scores of the source models are appended as the auxiliary features. The transfer parameters can be considered as the a part of the corresponding hyperplane. We can consider 2 augmentation strategies: multi-adaptation and single-adaptation.\relax }}{4}}
\newlabel{fig:aug}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Reasons for negative transfer}{4}}
\newlabel{eq:srm}{{3}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}SMTLe}{4}}
\newlabel{sec:smitle}{{IV}{4}}
\citation{kuzborskij2013stability}
\citation{elisseeff2003leave}
\citation{cawley2006leave}
\citation{crammer2002algorithmic}
\citation{BoydCO}
\citation{lampert2009learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Multiclass Prediction Loss with Leave-One-Out}{5}}
\newlabel{eq:linear}{{4}{5}}
\newlabel{eq:loo}{{5}{5}}
\newlabel{eq:solution}{{7}{5}}
\newlabel{eq:train_loss}{{8}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Loss Function of SMTLe}{5}}
\newlabel{eq:loss}{{9}{5}}
\newlabel{eq:dual}{{10}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiment}{5}}
\newlabel{sec:exp}{{V}{5}}
\citation{lecun1998gradient}
\citation{hull1994database}
\citation{jie2011multiclass}
\citation{tommasi2014learning}
\citation{aytar2011tabula}
\citation{kuzborskij2013n}
\citation{tommasi2014learning}
\citation{gehler2009feature}
\newlabel{alg:1}{{\caption@xref {alg:1}{ on input line 1}}{6}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces SMTLe\relax }}{6}}
\newlabel{alg:1}{{1}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Dataset}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Baseline methods and experiment setup}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Experiment results on 5 examples per class in target training set. We show the percentage of accuracy across the 10 classes on different noise level. We use "*" to denote the results suffer from negative transfer\relax }}{7}}
\newlabel{tab:rs}{{I}{7}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {Results on MNIST}}}{7}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {Results on USPS}}}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces We add the noise (Salt \& Pepper niose) to the source data to generate the source domain with different relatedness to the target domain. From the images we can see that the source domain is still related to the target domain in different level of the noise rate\relax }}{7}}
\newlabel{fig:noise}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Experiment result \& analysis}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{7}}
\citation{rockafellar2015convex}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Results on MNIST with 10/15/20/25 positive examples for each class\relax }}{8}}
\newlabel{tab:mnist}{{II}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {10 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {15 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(c)}{\ignorespaces {20 examples per class}}}{8}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(d)}{\ignorespaces {25 examples per class}}}{8}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  A: Convergence Analysis}{8}}
\newlabel{appd:convg}{{A}{8}}
\newlabel{eq:app:strong}{{12}{8}}
\newlabel{eq:app:inner}{{13}{8}}
\newlabel{eq:app:squrediff}{{14}{8}}
\newlabel{eq:app:it_diff}{{15}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Results on USPS with 10/15/20/25 positive examples for each class\relax }}{9}}
\newlabel{tab:usps}{{III}{9}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {10 examples per class}}}{9}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {15 examples per class}}}{9}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(c)}{\ignorespaces {20 examples per class}}}{9}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(d)}{\ignorespaces {25 examples per class}}}{9}}
\newlabel{eq:app:difsum}{{17}{9}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  B: Proof of avoiding negative transfer}{9}}
\newlabel{appd:proof}{{B}{9}}
\newlabel{eq:opt_beta}{{20}{9}}
\bibstyle{IEEEtran}
\bibdata{research}
\bibcite{ben2010theory}{1}
\bibcite{pan2010survey}{2}
\bibcite{tommasi2014learning}{3}
\bibcite{kuzborskij2013n}{4}
\bibcite{kuzborskij2013stability}{5}
\bibcite{bishop2006pattern}{6}
\bibcite{ben2007analysis}{7}
\bibcite{jie2011multiclass}{8}
\bibcite{Lu201514}{9}
\bibcite{lim2012transfer}{10}
\bibcite{LongICML15}{11}
\bibcite{yang2007cross}{12}
\bibcite{aytar2011tabula}{13}
\bibcite{davis2009deep}{14}
\bibcite{wang2014active}{15}
\bibcite{zhou2014multi}{16}
\bibcite{tommasi2010safety}{17}
\bibcite{cawley2006leave}{18}
\bibcite{vapnik2015learning}{19}
\bibcite{vapnik1999overview}{20}
\bibcite{suykens1999least}{21}
\bibcite{elisseeff2003leave}{22}
\bibcite{crammer2002algorithmic}{23}
\bibcite{BoydCO}{24}
\bibcite{lampert2009learning}{25}
\bibcite{lecun1998gradient}{26}
\bibcite{hull1994database}{27}
\bibcite{gehler2009feature}{28}
\bibcite{rockafellar2015convex}{29}
\newlabel{eq:link1}{{21}{10}}
\@writefile{toc}{\contentsline {section}{References}{10}}
