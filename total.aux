\relax 
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{fei2006one}
\citation{kuzborskij2013stability}
\citation{kuzborskij2013stability}
\citation{tommasi2014learning}
\citation{ben2010theory}
\citation{ben2007analysis}
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\citation{jie2011multiclass}
\citation{Lu201514}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Relying on the related source knowledge can improve the performance of the target model while forcing the target model to rely on the unrelated source could suffer from negative transfer.\relax }}{1}}
\citation{pan2010survey}
\citation{tommasi2014learning}
\citation{lim2012transfer}
\citation{LongICML15}
\citation{yang2007cross}
\citation{aytar2011tabula}
\citation{tommasi2014learning}
\citation{kuzborskij2013n}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Negative transfer happens when we transfer source hypothesis $f'$ to target one. Points with different color represent different categories. The data distribution would change when a new category is added into the dataset. The newly added category (red points) can also greatly affect the data distribution in target task and negative transfer could happen when we consider the multi-class scenario. \relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:distribution}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}}
\newlabel{sec:work}{{II}{2}}
\citation{davis2009deep}
\citation{wang2014active}
\citation{zhou2014multi}
\citation{kuzborskij2013n}
\citation{tommasi2010safety}
\citation{cawley2006leave}
\citation{tommasi2014learning}
\@writefile{toc}{\contentsline {section}{\numberline {III}Insight into negative transfer}{3}}
\newlabel{sec:prob}{{III}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Biased regularization in HTL}{3}}
\newlabel{eq:svm_obj}{{1}{3}}
\newlabel{eq:opt}{{2}{3}}
\newlabel{eq:solu}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Data Augumentation in HTL}{3}}
\newlabel{eq:aug_pre}{{4}{3}}
\newlabel{eq:aug}{{5}{3}}
\citation{vapnik1999overview}
\citation{suykens1999least}
\bibstyle{IEEEtran}
\bibdata{research}
\bibcite{pan2010survey}{1}
\bibcite{tommasi2014learning}{2}
\bibcite{fei2006one}{3}
\bibcite{kuzborskij2013stability}{4}
\bibcite{ben2010theory}{5}
\bibcite{ben2007analysis}{6}
\bibcite{kuzborskij2013n}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The transfer learning process can be considered as the augmentation of the target data where the decision scores of the source models are appended as the auxiliary features. The transfer parameters can be considered as the a part of the corresponding hyperplane.\relax }}{4}}
\newlabel{fig:aug}{{3}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Reasons for negative transfer}{4}}
\newlabel{eq:srm}{{6}{4}}
\newlabel{eq:asvm}{{7}{4}}
\newlabel{eq:linear}{{8}{4}}
\newlabel{eq:solution}{{10}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}SMTLe}{4}}
\newlabel{sec:smitle}{{IV}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Experiment}{4}}
\newlabel{sec:exp}{{V}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}}
\bibcite{jie2011multiclass}{8}
\bibcite{Lu201514}{9}
\bibcite{lim2012transfer}{10}
\bibcite{LongICML15}{11}
\bibcite{yang2007cross}{12}
\bibcite{aytar2011tabula}{13}
\bibcite{davis2009deep}{14}
\bibcite{wang2014active}{15}
\bibcite{zhou2014multi}{16}
\bibcite{tommasi2010safety}{17}
\bibcite{cawley2006leave}{18}
\bibcite{vapnik1999overview}{19}
\bibcite{suykens1999least}{20}
